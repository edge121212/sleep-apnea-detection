"""
my_apnea_processing.py

Apnea-ECG data preprocessing:
 - Local alignment of R-peak indices (RA, RRI, RRID) in each 6-min segment
 - Record-level splitting into train, val, test sets
 - Saves output as .npy
"""

import os
import random
import numpy as np
import wfdb
import pywt
from scipy import signal

# ============================
# PART A: DATA READING
# ============================

def load_apnea_ecg_record(record_path):
    """
    Read a single record (ECG) from Apnea-ECG Database using WFDB.
    Returns:
      ecg (np.ndarray): shape (num_samples,)
      fs (int or float): sampling frequency
    """
    signals, fields = wfdb.rdsamp(record_path)
    ecg = signals[:, 0]  # single-lead ECG
    fs = fields["fs"]    # typically 100 Hz
    return ecg, fs


def load_apnea_annotations(ann_path):
    """
    Read minute-by-minute apnea annotations (apn).
    Returns:
      labels (np.ndarray of 0/1), length = # minutes
    """
    ann = wfdb.rdann(ann_path, "apn")
    labels = np.array([1 if s == "A" else 0 for s in ann.symbol], dtype=int)
    return labels

# ============================
# PART B: WAVELET DENOISING
# ============================

def wavelet_denoise(ecg, wavelet="coif4", level=6):
    """
    Remove high freq (~25-50Hz) and very low freq (<1Hz) by zeroing cD1 & cA6.
    """
    coeffs = pywt.wavedec(ecg, wavelet, level=level)
    # cA6 => coeffs[0], cD1 => coeffs[-1]
    coeffs[0]  = np.zeros_like(coeffs[0])
    coeffs[-1] = np.zeros_like(coeffs[-1])
    denoised = pywt.waverec(coeffs, wavelet)
    # match original length
    return denoised[: len(ecg)]

# ============================
# PART C: NORMALIZATION
# ============================

def zscore_normalize(ecg):
    mu = np.mean(ecg)
    sigma = np.std(ecg)
    if sigma < 1e-12:
        return ecg - mu
    return (ecg - mu) / sigma

def minmax_normalize(signal):
    """
    Min-Max æ¨™æº–åŒ–ï¼šå°‡ä¿¡è™Ÿç¸®æ”¾åˆ° [0, 1] ç¯„åœ
    å°æ¯å€‹ segment ç¨ç«‹é€²è¡Œæ¨™æº–åŒ–
    """
    min_val = np.min(signal)
    max_val = np.max(signal)
    
    # é¿å…é™¤é›¶éŒ¯èª¤
    if max_val - min_val < 1e-12:
        return signal - min_val  # å¦‚æœä¿¡è™Ÿæ˜¯å¸¸æ•¸ï¼Œè¿”å›é›¶ä¿¡è™Ÿ
    
    return (signal - min_val) / (max_val - min_val)

# ============================
# PART D: R-PEAK DETECTION, AUX
# ============================

def detect_rpeaks(ecg, fs):
    """
    ä½¿ç”¨ biosppy Hamilton æ¼”ç®—æ³•æª¢æ¸¬ R å³°
    """
    print(f"Rå³°æª¢æ¸¬ - ä¿¡è™Ÿé•·åº¦: {len(ecg)}, æ¡æ¨£ç‡: {fs} Hz")
    
    from biosppy.signals.ecg import correct_rpeaks, hamilton_segmenter
    print("ä½¿ç”¨ biosppy Hamilton æ¼”ç®—æ³•...")
    
    # Hamilton æ¼”ç®—æ³•æª¢æ¸¬ R å³°
    rpeaks = hamilton_segmenter(ecg, sampling_rate=fs)[0]
    print(f"   åˆå§‹æª¢æ¸¬åˆ° {len(rpeaks)} å€‹ R å³°")
    
    # ä¿®æ­£ R å³°ä½ç½®
    rpeaks_corrected = correct_rpeaks(signal=ecg, 
                                    rpeaks=rpeaks, 
                                    sampling_rate=fs, 
                                    tol=0.05)[0]
    print(f"   ä¿®æ­£å¾Œæœ‰ {len(rpeaks_corrected)} å€‹ R å³°")
    
    # è¨ˆç®—å¹³å‡å¿ƒç‡
    if len(rpeaks_corrected) > 1:
        avg_rr = np.mean(np.diff(rpeaks_corrected)) / fs
        avg_hr = 60 / avg_rr
        print(f"   å¹³å‡å¿ƒç‡: {avg_hr:.1f} bpm")
    
    return rpeaks_corrected

def construct_ra_rri_rrid(ecg, rpeaks, fs):

    rpeak_times = rpeaks / fs
    ra_values   = ecg[rpeaks]         # amplitude at each R-peak
    rr_intervals= np.diff(rpeak_times)
    rrid        = np.diff(rr_intervals)
    return ra_values, rr_intervals, rrid

# ============================
# PART E: SEGMENTING INTO 6-MIN WINDOWS
# ============================

def segment_ecg_with_context(ecg, fs, labels=None):
    """
    Return a list of (ecg_segment, label, start_sample, end_sample).
    Each segment = 6 min => from [m_idx-2.5..m_idx+3.5).
    If labels is None, generate segments for every minute.
    """
    segments = []
    n_samples = len(ecg)
    minute_len = 60 * fs  # Number of samples in one minute

    if labels is not None:
        # Generate labeled segments
        for m_idx, lbl in enumerate(labels):
            start_sample = int((m_idx - 2.5) * minute_len)
            end_sample = int((m_idx + 3.5) * minute_len)
            if start_sample < 0 or end_sample > n_samples:
                continue
            seg = ecg[start_sample:end_sample]
            segments.append((seg, lbl, start_sample, end_sample))
    else:
        # Generate unlabeled segments for every minute
        num_minutes = n_samples // minute_len
        for m_idx in range(num_minutes):
            start_sample = int((m_idx - 2.5) * minute_len)
            end_sample = int((m_idx + 3.5) * minute_len)
            if start_sample < 0 or end_sample > n_samples:
                continue
            seg = ecg[start_sample:end_sample]
            segments.append((seg, None, start_sample, end_sample))

    return segments

# ============================
# PART F: INTERPOLATION, AUX timeseries
# ============================

def interp_to_length(signal_in, out_len=1024):
    """ Resample 1D array to out_len using FFT-based method """
    return signal.resample(signal_in, out_len)

def build_ra_timeseries(local_ecg, local_rpeaks, length):
    """
    local_ecg: shape (length,) - the 6-min ECG segment
    local_rpeaks: list of R-peak indices in [0, length)
    returns array shape (length,) with amplitude at local_rpeaks
    """
    ra = np.zeros(length)
    for rp in local_rpeaks:
        if 0 <= rp < length:
            ra[rp] = local_ecg[rp]
    return ra

def build_rri_timeseries(local_rpeaks, fs, length):
    """
    For each consecutive R-peaks [i-1, i], fill from [start..end]
    with the same RR interval in seconds. 
    local_rpeaks: sorted list of indices in [0, length)
    """
    rri_series = np.zeros(length)
    for i in range(1, len(local_rpeaks)):
        start = local_rpeaks[i-1]
        end   = local_rpeaks[i]
        rr_val = (end - start) / fs
        if end <= length:
            rri_series[start:end] = rr_val
    return rri_series

def build_rrid_timeseries(local_rpeaks, fs, length):
    """
    first difference of RRI. We'll store it at the R-peaks themselves 
    or fill from [rpeaks[i]..rpeaks[i+1]] with the same RRID, your choice.
    This is a naive approach.
    """
    intervals = []
    for i in range(1, len(local_rpeaks)):
        delta = (local_rpeaks[i] - local_rpeaks[i-1]) / fs
        intervals.append(delta)
    diffs = np.diff(intervals)

    rrid_series = np.zeros(length)
    # place each diff at the peak index i
    for i in range(1, len(intervals)-1):
        idx = local_rpeaks[i]
        if idx < length:
            rrid_series[idx] = diffs[i-1]
    return rrid_series

def preprocess_record(record_id, base_dir, out_len=1024):
    """Modified to return record_id with each segment"""
    print(f"\nè™•ç†è¨˜éŒ„: {record_id}")
    
    rec_path = os.path.join(base_dir, record_id)
    ecg_raw, fs = load_apnea_ecg_record(rec_path)
    print(f"   åŸå§‹ ECG é•·åº¦: {len(ecg_raw)} æ¨£æœ¬ ({len(ecg_raw)/fs/60:.1f} åˆ†é˜)")

    # Only do wavelet denoising, remove zscore normalization
    ecg_denoised = wavelet_denoise(ecg_raw, "coif4", 6)

    ann_path = os.path.join(base_dir, record_id)
    try:
        labels = load_apnea_annotations(ann_path)
        print(f"   è¼‰å…¥æ¨™è¨»: {len(labels)} åˆ†é˜, {sum(labels)} å€‹å‘¼å¸ä¸­æ­¢äº‹ä»¶")
    except:
        labels = None
        print("   è­¦å‘Š: ç„¡æ¨™è¨»æ–‡ä»¶")

    # Use denoised ECG (not normalized) for r-peak detection
    rpeaks = detect_rpeaks(ecg_denoised, fs)

    data_segments = []
    segments = segment_ecg_with_context(ecg_denoised, fs, labels)
    print(f"   ç”Ÿæˆ {len(segments)} å€‹ 6 åˆ†é˜æ®µ")
    print(f"   ğŸ“Š å°‡å°æ¯å€‹ segment é€²è¡Œ Min-Max æ¨™æº–åŒ– [0,1]")
    
    for seg_ecg, seg_lbl, start_sample, end_sample in segments:
        seg_len = len(seg_ecg)
        # Resample the local ECG to out_len
        ecg_resamp = interp_to_length(seg_ecg, out_len)

        # 1) Build local r-peaks in [0, seg_len)
        local_rpeaks = [rp - start_sample for rp in rpeaks
                        if (rp >= start_sample) and (rp < end_sample)]

        # === æ–°å¢ï¼šéæ¿¾ç•°å¸¸ RR interval çš„ R-peak ===
        if len(local_rpeaks) > 1:
            rr_intervals = np.diff(local_rpeaks) / fs  # å–®ä½ï¼šç§’
            hr = 60 / rr_intervals
            # åˆç† HR ç¯„åœ 40~180
            valid_idx = np.where((hr >= 40) & (hr <= 180))[0]
            # åªä¿ç•™åˆç†çš„ R-peakï¼ˆæ³¨æ„ diff æœƒå°‘ä¸€å€‹ï¼Œéœ€è£œå›ç¬¬ä¸€å€‹ R-peakï¼‰
            filtered_rpeaks = [local_rpeaks[0]] + [local_rpeaks[i+1] for i in valid_idx]
        else:
            filtered_rpeaks = local_rpeaks

        # 2) RA, RRI, RRID as local time series of length seg_len
        RA_ts  = build_ra_timeseries(seg_ecg, filtered_rpeaks, seg_len)
        RRI_ts = build_rri_timeseries(filtered_rpeaks, fs, seg_len)
        RRID_ts= build_rrid_timeseries(filtered_rpeaks, fs, seg_len)

        # 3) Resample each to out_len
        RA_resamp   = interp_to_length(RA_ts,   out_len)
        RRI_resamp  = interp_to_length(RRI_ts,  out_len)
        RRID_resamp = interp_to_length(RRID_ts, out_len)

        # ğŸ”§ æ–°å¢ï¼šå°æ¯å€‹é€šé“é€²è¡Œ Min-Max æ¨™æº–åŒ– (segment-wise)
        ecg_normalized = minmax_normalize(ecg_resamp)
        RA_normalized = minmax_normalize(RA_resamp)
        RRI_normalized = minmax_normalize(RRI_resamp)
        RRID_normalized = minmax_normalize(RRID_resamp)

        # stack => shape (out_len, 4) ä½¿ç”¨æ¨™æº–åŒ–å¾Œçš„æ•¸æ“š
        seg_4ch = np.vstack([ecg_normalized, RA_normalized, RRI_normalized, RRID_normalized]).T
        
        # ğŸ” é©—è­‰æ¨™æº–åŒ–æ•ˆæœï¼ˆå¯é¸çš„èª¿è©¦ä¿¡æ¯ï¼‰
        if len(data_segments) == 0:  # åªåœ¨ç¬¬ä¸€å€‹ segment æ™‚é¡¯ç¤º
            print(f"   ğŸ“Š æ¨™æº–åŒ–ç¯„åœé©—è­‰ (ç¬¬ä¸€å€‹segment):")
            print(f"      ECG: [{ecg_normalized.min():.3f}, {ecg_normalized.max():.3f}]")
            print(f"      RA:  [{RA_normalized.min():.3f}, {RA_normalized.max():.3f}]")
            print(f"      RRI: [{RRI_normalized.min():.3f}, {RRI_normalized.max():.3f}]")
            print(f"      RRID:[{RRID_normalized.min():.3f}, {RRID_normalized.max():.3f}]")
        
        # Add record_id to the returned tuple
        data_segments.append((seg_4ch, seg_lbl, record_id))

    print(f"   å®Œæˆè™•ç†ï¼Œè¼¸å‡º {len(data_segments)} å€‹æ•¸æ“šæ®µ")
    return data_segments

# =====================
# PART G: RECORD-LEVEL SPLIT
# =====================

def process_segments_segment_level(base_dir, out_len=1024):
    """
    Process all segments and save with patient IDs
    åŒ…å« segment-wise Min-Max æ¨™æº–åŒ– [0,1]
    """
    output_dir = os.path.join(base_dir, "split_data")
    
    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨è™•ç†éçš„æ•¸æ“š
    if os.path.exists(output_dir):
        existing_files = [f for f in os.listdir(output_dir) if f.endswith('.npy')]
        if existing_files:
            print(f"\nè­¦å‘Š: ç™¼ç¾å·²å­˜åœ¨çš„è™•ç†æ•¸æ“šåœ¨ '{output_dir}'")
            print(f"ç¾æœ‰æ–‡ä»¶: {existing_files}")
            
            response = input("æ˜¯å¦è¦†è“‹ç¾æœ‰æ•¸æ“š? (y/n, é è¨­ç‚º n): ").strip().lower()
            if response not in ['y', 'yes']:
                print("å–æ¶ˆè™•ç†ï¼Œä¿ç•™ç¾æœ‰æ•¸æ“š")
                return
            else:
                print("å°‡è¦†è“‹ç¾æœ‰æ•¸æ“š...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # The 35 labeled records
    labeled_records = (
        [f"a{i:02d}" for i in range(1, 21)] +
        [f"b{i:02d}" for i in range(1, 6)] +
        [f"c{i:02d}" for i in range(1, 11)]
    )
    
    test_records = [f"x{i:02d}" for i in range(1, 36)]
    
    print("Processing labeled records:", labeled_records)
    print("Processing test records:", test_records)
    
    # Process labeled records (for train and val)
    all_segments = []
    for rec_id in labeled_records:
        print(f"Processing record: {rec_id}")
        segs = preprocess_record(rec_id, base_dir, out_len=out_len)
        all_segments.extend(segs)
        print(f"Processed {len(segs)} segments from {rec_id}")
    
    print(f"Total segments from labeled records: {len(all_segments)}")
    
    # Process test records
    test_segments = []
    for rec_id in test_records:
        print(f"Processing test record: {rec_id}")
        segs = preprocess_record(rec_id, base_dir, out_len=out_len)
        test_segments.extend(segs)
        print(f"Processed {len(segs)} segments from {rec_id}")
    
    print(f"Total segments from test records: {len(test_segments)}")
    
    # Shuffle and split labeled data into train/val
    random.shuffle(all_segments)
    n_total = len(all_segments)
    n_train = int(0.90 * n_total)
    
    # Split the data
    train_data = all_segments[:n_train]
    val_data = all_segments[n_train:]
    test_data = test_segments
    
    # Convert to arrays and save
    trainX, trainY, trainIDs = to_xy(train_data)
    valX, valY, valIDs = to_xy(val_data)
    testX, testY, testIDs = to_xy(test_data)
    
    # Save training data
    print(f"Saving training data: {len(train_data)} segments")
    np.save(os.path.join(output_dir, "trainX.npy"), trainX)
    np.save(os.path.join(output_dir, "trainY.npy"), trainY)
    np.save(os.path.join(output_dir, "trainIDs.npy"), trainIDs)
    
    # Save validation data
    print(f"Saving validation data: {len(val_data)} segments")
    np.save(os.path.join(output_dir, "valX.npy"), valX)
    np.save(os.path.join(output_dir, "valY.npy"), valY)
    np.save(os.path.join(output_dir, "valIDs.npy"), valIDs)
    
    # Save test data
    print(f"Saving test data: {len(test_data)} segments")
    np.save(os.path.join(output_dir, "testX.npy"), testX)
    np.save(os.path.join(output_dir, "testY.npy"), testY)
    np.save(os.path.join(output_dir, "testIDs.npy"), testIDs)
    
    print(f"\nAll data saved in '{output_dir}':")
    print(f"Train: {len(train_data)} segments")
    print(f"Validation: {len(val_data)} segments")
    # print(f"Test: {len(test_data)} segments")

def to_xy(data_list):
    """Helper to convert list of tuples to numpy arrays"""
    if not data_list:
        return np.array([]), np.array([]), np.array([])
    
    try:
        # Each segment => shape (1024,4)
        X = np.stack([seg for seg, _, _ in data_list], axis=0)
        Y = np.array([lbl for _, lbl, _ in data_list], dtype=int)
        IDs = np.array([pid for _, _, pid in data_list])
        return X, Y, IDs
    except Exception as e:
        print(f"Error in to_xy conversion: {e}")
        raise

# =====================
# PART H: æ•¸æ“šæª¢æŸ¥å’Œè¼‰å…¥
# =====================

def check_existing_data(base_dir):
    """æª¢æŸ¥æ˜¯å¦å·²æœ‰è™•ç†å®Œçš„æ•¸æ“š"""
    output_dir = os.path.join(base_dir, "split_data")
    
    if not os.path.exists(output_dir):
        return False, "ç›®éŒ„ä¸å­˜åœ¨"
    
    required_files = [
        "trainX.npy", "trainY.npy", "trainIDs.npy",
        "valX.npy", "valY.npy", "valIDs.npy",
        "testX.npy", "testY.npy", "testIDs.npy"
    ]
    
    existing_files = []
    missing_files = []
    
    for file in required_files:
        filepath = os.path.join(output_dir, file)
        if os.path.exists(filepath):
            existing_files.append(file)
        else:
            missing_files.append(file)
    
    if len(existing_files) == len(required_files):
        return True, f"å®Œæ•´æ•¸æ“šé›†å·²å­˜åœ¨ ({len(existing_files)} å€‹æ–‡ä»¶)"
    elif existing_files:
        return False, f"éƒ¨åˆ†æ•¸æ“šå­˜åœ¨: {existing_files}, ç¼ºå°‘: {missing_files}"
    else:
        return False, "ç„¡æ•¸æ“šæ–‡ä»¶"

def load_existing_data(base_dir):
    """è¼‰å…¥å·²å­˜åœ¨çš„è™•ç†æ•¸æ“š"""
    output_dir = os.path.join(base_dir, "split_data")
    
    try:
        trainX = np.load(os.path.join(output_dir, "trainX.npy"))
        trainY = np.load(os.path.join(output_dir, "trainY.npy"))
        valX = np.load(os.path.join(output_dir, "valX.npy"))
        valY = np.load(os.path.join(output_dir, "valY.npy"))
        testX = np.load(os.path.join(output_dir, "testX.npy"))
        testY = np.load(os.path.join(output_dir, "testY.npy"))
        
        print(f"æˆåŠŸè¼‰å…¥ç¾æœ‰æ•¸æ“š:")
        print(f"  è¨“ç·´é›†: {trainX.shape[0]} æ¨£æœ¬")
        print(f"  é©—è­‰é›†: {valX.shape[0]} æ¨£æœ¬") 
        print(f"  æ¸¬è©¦é›†: {testX.shape[0]} æ¨£æœ¬")
        print(f"  æ•¸æ“šå½¢ç‹€: {trainX.shape}")
        
        return {
            'trainX': trainX, 'trainY': trainY,
            'valX': valX, 'valY': valY,
            'testX': testX, 'testY': testY
        }
        
    except Exception as e:
        print(f"è¼‰å…¥æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

# =====================
# MAIN
# =====================

if __name__ == "__main__":
    base_dir = r"C:\python\Apnea\physionet"  # path where a01.dat, a01.hea, etc. are located
    
    # å…ˆæª¢æŸ¥æ˜¯å¦å·²æœ‰è™•ç†å¥½çš„æ•¸æ“š
    has_data, status = check_existing_data(base_dir)
    print(f"æ•¸æ“šæª¢æŸ¥çµæœ: {status}")
    
    if has_data:
        print("\nç™¼ç¾å®Œæ•´çš„è™•ç†æ•¸æ“šï¼")
        print("é¸æ“‡æ“ä½œ:")
        print("1. è¼‰å…¥ç¾æœ‰æ•¸æ“š (æ¨è–¦)")
        print("2. é‡æ–°è™•ç†æ•¸æ“š (æœƒè¦†è“‹ç¾æœ‰)")
        print("3. å–æ¶ˆ")
        
        choice = input("è«‹é¸æ“‡ (1/2/3, é è¨­ç‚º 1): ").strip()
        
        if choice == "2":
            print("é–‹å§‹é‡æ–°è™•ç†æ•¸æ“š...")
            process_segments_segment_level(base_dir, out_len=1024)
        elif choice == "3":
            print("å–æ¶ˆæ“ä½œ")
        else:
            print("è¼‰å…¥ç¾æœ‰æ•¸æ“š...")
            data = load_existing_data(base_dir)
            if data:
                print("æ•¸æ“šè¼‰å…¥å®Œæˆï¼Œå¯ä»¥é–‹å§‹è¨“ç·´æ¨¡å‹ï¼")
    else:
        print(f"éœ€è¦è™•ç†æ•¸æ“š: {status}")
        print("é–‹å§‹æ•¸æ“šè™•ç†...")
        process_segments_segment_level(base_dir, out_len=1024)
