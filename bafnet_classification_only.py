#!/usr/bin/env python
# coding: utf-8

import time
import pickle
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
from scipy.interpolate import splev, splrep
from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
import math
from tqdm import tqdm

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class ScaledDotProductAttention(nn.Module):
    def __init__(self, return_attention=False, history_only=False):
        super(ScaledDotProductAttention, self).__init__()
        self.return_attention = return_attention
        self.history_only = history_only
        self.intensity = None
        self.attention = None

    def forward(self, query, key, value, mask=None):
        """
        ç°¡åŒ–çš„æ³¨æ„åŠ›æ©Ÿåˆ¶ - è™•ç†ç¶­åº¦ä¸åŒ¹é…å•é¡Œ
        Args:
            query: [batch, channels, time]
            key: [batch, channels, time] 
            value: [batch, channels, time]
        """
        # ğŸ”§ è§£æ±ºæ–¹æ¡ˆ1: ç¢ºä¿æ‰€æœ‰è¼¸å…¥æœ‰ç›¸åŒçš„æ™‚é–“ç¶­åº¦
        batch_size, channels, time_steps = query.shape
        
        # å¦‚æœ key å’Œ value çš„æ™‚é–“ç¶­åº¦ä¸åŒï¼Œä½¿ç”¨è‡ªé©æ‡‰æ± åŒ–èª¿æ•´
        if key.shape[2] != time_steps:
            key = F.adaptive_avg_pool1d(key, time_steps)
        if value.shape[2] != time_steps:
            value = F.adaptive_avg_pool1d(value, time_steps)
            
        # ğŸ”§ è§£æ±ºæ–¹æ¡ˆ2: ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–ç°¡åŒ–æ³¨æ„åŠ›
        # å°‡æ™‚é–“ç¶­åº¦æ± åŒ–ï¼Œå°ˆæ³¨æ–¼é€šé“é–“çš„æ³¨æ„åŠ›
        query_pooled = F.adaptive_avg_pool1d(query, 1).squeeze(-1)  # [batch, channels]
        key_pooled = F.adaptive_avg_pool1d(key, 1).squeeze(-1)      # [batch, channels]
        value_pooled = F.adaptive_avg_pool1d(value, 1).squeeze(-1)  # [batch, channels]
        
        # è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡ [batch, channels, channels]
        attention_scores = torch.matmul(query_pooled.unsqueeze(2), key_pooled.unsqueeze(1))
        attention_scores = attention_scores / math.sqrt(channels)
        
        # è»Ÿæœ€å¤§åŒ–
        attention_weights = F.softmax(attention_scores, dim=-1)  # [batch, channels, channels]
        
        # æ‡‰ç”¨æ³¨æ„åŠ›åˆ° value
        attended_value = torch.matmul(attention_weights, value_pooled.unsqueeze(-1)).squeeze(-1)  # [batch, channels]
        
        # ğŸ”§ è§£æ±ºæ–¹æ¡ˆ3: å°‡æ³¨æ„åŠ›çµæœå»£æ’­å›åŸå§‹æ™‚é–“ç¶­åº¦
        # ä½¿ç”¨é€šé“æ³¨æ„åŠ›çš„çµæœä¾†é‡æ–°åŠ æ¬ŠåŸå§‹çš„ value
        attention_mask = F.sigmoid(attended_value).unsqueeze(-1)  # [batch, channels, 1]
        output = value * attention_mask  # [batch, channels, time]
        
        if self.return_attention:
            return output, attention_weights
        return output

class BAFNet_ClassificationOnly(nn.Module):
    """
    ç´”åˆ†é¡ç‰ˆæœ¬çš„ BAFNet - ç§»é™¤é‡å»ºéƒ¨åˆ†ï¼Œå°ˆæ³¨æ–¼åˆ†é¡ä»»å‹™
    ğŸ›¡ï¸ æ·»åŠ å¤šå±¤æ¬¡æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ
    """
    def __init__(self, input_shape, weight=1e-3, dropout_rate=0.3):
        super(BAFNet_ClassificationOnly, self).__init__()
        
        # ä¿®æ”¹ï¼šé©é…4å€‹é€šé“ (ECG, RA, RRI, RRID)
        # Initial convolutions - åˆ†åˆ¥è™•ç†å››å€‹é€šé“
        self.conv1_ch1 = nn.Conv1d(1, 16, kernel_size=11, stride=1, padding=5, bias=True)  # ECG
        self.conv1_ch2 = nn.Conv1d(1, 16, kernel_size=11, stride=1, padding=5, bias=True)  # RA
        self.conv1_ch3 = nn.Conv1d(1, 16, kernel_size=11, stride=1, padding=5, bias=True)  # RRI
        self.conv1_ch4 = nn.Conv1d(1, 16, kernel_size=11, stride=1, padding=5, bias=True)  # RRID
        
        # ğŸ›¡ï¸ æ·»åŠ  BatchNorm å’Œ Dropout åˆ°å·ç©å±¤
        self.bn1_ch1 = nn.BatchNorm1d(16)
        self.bn1_ch2 = nn.BatchNorm1d(16)
        self.bn1_ch3 = nn.BatchNorm1d(16)
        self.bn1_ch4 = nn.BatchNorm1d(16)
        self.dropout_conv1 = nn.Dropout1d(dropout_rate * 0.5)  # å·ç©å±¤è¼ƒä½ dropout
        
        # Second layer convolutions
        self.conv2_ch1 = nn.Conv1d(16, 24, kernel_size=11, stride=2, padding=5, bias=True)
        self.conv2_ch2 = nn.Conv1d(16, 24, kernel_size=11, stride=2, padding=5, bias=True)
        self.conv2_ch3 = nn.Conv1d(16, 24, kernel_size=11, stride=2, padding=5, bias=True)
        self.conv2_ch4 = nn.Conv1d(16, 24, kernel_size=11, stride=2, padding=5, bias=True)
        self.pool1 = nn.MaxPool1d(3, padding=1)
        
        # ğŸ›¡ï¸ ç¬¬äºŒå±¤ BatchNorm
        self.bn2_ch1 = nn.BatchNorm1d(24)
        self.bn2_ch2 = nn.BatchNorm1d(24)
        self.bn2_ch3 = nn.BatchNorm1d(24)
        self.bn2_ch4 = nn.BatchNorm1d(24)
        self.dropout_conv2 = nn.Dropout1d(dropout_rate * 0.6)
        
        # Third layer convolutions
        self.conv3_ch1 = nn.Conv1d(24, 32, kernel_size=11, stride=1, padding=5, bias=True)
        self.conv3_ch2 = nn.Conv1d(24, 32, kernel_size=11, stride=1, padding=5, bias=True)
        self.conv3_ch3 = nn.Conv1d(24, 32, kernel_size=11, stride=1, padding=5, bias=True)
        self.conv3_ch4 = nn.Conv1d(24, 32, kernel_size=11, stride=1, padding=5, bias=True)
        self.pool2 = nn.MaxPool1d(5, padding=2)
        
        # ğŸ›¡ï¸ ç¬¬ä¸‰å±¤ BatchNorm
        self.bn3_ch1 = nn.BatchNorm1d(32)
        self.bn3_ch2 = nn.BatchNorm1d(32)
        self.bn3_ch3 = nn.BatchNorm1d(32)
        self.bn3_ch4 = nn.BatchNorm1d(32)
        self.dropout_conv3 = nn.Dropout1d(dropout_rate * 0.7)
        
        # ä¿®æ”¹ï¼šèåˆå±¤é©é…4å€‹é€šé“ (24*4 = 96)
        self.conv_fusion = nn.Conv1d(96, 32, kernel_size=11, stride=1, padding=5, bias=True)
        self.bn_fusion = nn.BatchNorm1d(32)
        
        # Attention layers
        self.attention = ScaledDotProductAttention()
        
        # ğŸ—‘ï¸ ç§»é™¤ï¼šå®Œå…¨ç§»é™¤é‡å»ºéƒ¨åˆ† (up1_ch1, up1_ch2, up1_ch3, up1_ch4)
        # é€™äº›ä½”äº†å¤§é‡åƒæ•¸ä½†å°åˆ†é¡æ²’æœ‰ç›´æ¥å¹«åŠ©
        
        # âœ¨ å¢å¼·ï¼šæ›´å¼·çš„åˆ†é¡é ­éƒ¨ + å¤šå±¤æ¬¡æ­£å‰‡åŒ–
        self.fc1 = nn.Linear(128, 128, bias=True)
        self.bn_fc1 = nn.BatchNorm1d(128)  # ğŸ›¡ï¸ å…¨é€£æ¥å±¤ä¹ŸåŠ  BatchNorm
        self.dropout1 = nn.Dropout(dropout_rate * 1.2)  # æ›´é«˜çš„ dropout
        
        self.fc2 = nn.Linear(128, 64, bias=True)
        self.bn_fc2 = nn.BatchNorm1d(64)
        self.dropout2 = nn.Dropout(dropout_rate)
        
        # Classification head - æ›´æ·±çš„åˆ†é¡å™¨
        self.classifier = nn.Sequential(
            nn.Linear(64, 32, bias=True),
            nn.BatchNorm1d(32),  # ğŸ›¡ï¸ åˆ†é¡å™¨ä¹ŸåŠ  BatchNorm
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.8),
            nn.Linear(32, 2, bias=True)
        )
        
        # Store weight decay parameter
        self.weight_decay = weight
        self.dropout_rate = dropout_rate
        
    def forward(self, x):
        # æ·»åŠ  Min-Max æ¨™æº–åŒ–ï¼ˆèˆ‡ YourModel ä¸€è‡´ï¼‰
        x_normalized = torch.zeros_like(x)
        for i in range(4):
            channel_data = x[:, :, i]
            min_val = channel_data.min(dim=1, keepdim=True)[0]
            max_val = channel_data.max(dim=1, keepdim=True)[0]
            range_val = max_val - min_val + 1e-8
            x_normalized[:, :, i] = (channel_data - min_val) / range_val
    
        # ç·¨ç¢¼å™¨éƒ¨åˆ†ï¼ˆåŠ å…¥æ­£å‰‡åŒ–ï¼‰
        x = x_normalized.permute(0, 2, 1)  # [batch_size, 4, 1024]
        
        # åˆ†é›¢å››å€‹é€šé“
        ch1 = x[:, 0:1, :]  # ECG   [batch_size, 1, 1024]
        ch2 = x[:, 1:2, :]  # RA    [batch_size, 1, 1024]
        ch3 = x[:, 2:3, :]  # RRI   [batch_size, 1, 1024]
        ch4 = x[:, 3:4, :]  # RRID  [batch_size, 1, 1024]
        
        # ğŸ›¡ï¸ ç¬¬ä¸€å±¤ï¼šå·ç© + BatchNorm + ReLU + Dropout
        x1 = F.relu(self.bn1_ch1(self.conv1_ch1(ch1)))
        x2 = F.relu(self.bn1_ch2(self.conv1_ch2(ch2)))
        x3 = F.relu(self.bn1_ch3(self.conv1_ch3(ch3)))
        x4 = F.relu(self.bn1_ch4(self.conv1_ch4(ch4)))
        
        # æ‡‰ç”¨ç¬¬ä¸€å±¤ dropout
        x1 = self.dropout_conv1(x1)
        x2 = self.dropout_conv1(x2)
        x3 = self.dropout_conv1(x3)
        x4 = self.dropout_conv1(x4)
        
        # ğŸ›¡ï¸ ç¬¬äºŒå±¤ï¼šå·ç© + BatchNorm + ReLU + Pool + Dropout
        x1 = F.relu(self.bn2_ch1(self.conv2_ch1(x1)))
        x1 = self.pool1(x1)
        x2 = F.relu(self.bn2_ch2(self.conv2_ch2(x2)))
        x2 = self.pool1(x2)
        x3 = F.relu(self.bn2_ch3(self.conv2_ch3(x3)))
        x3 = self.pool1(x3)
        x4 = F.relu(self.bn2_ch4(self.conv2_ch4(x4)))
        x4 = self.pool1(x4)
        
        # æ‡‰ç”¨ç¬¬äºŒå±¤ dropout
        x1 = self.dropout_conv2(x1)
        x2 = self.dropout_conv2(x2)
        x3 = self.dropout_conv2(x3)
        x4 = self.dropout_conv2(x4)
        
        # ç¬¬äºŒå±¤èåˆ
        fsn2 = torch.cat([x1, x2, x3, x4], dim=1)  # [batch_size, 96, time]
        
        # ğŸ›¡ï¸ ç¬¬ä¸‰å±¤ï¼šå·ç© + BatchNorm + ReLU + Pool + Dropout
        x1 = F.relu(self.bn3_ch1(self.conv3_ch1(x1)))
        x1 = self.pool2(x1)
        x2 = F.relu(self.bn3_ch2(self.conv3_ch2(x2)))
        x2 = self.pool2(x2)
        x3 = F.relu(self.bn3_ch3(self.conv3_ch3(x3)))
        x3 = self.pool2(x3)
        x4 = F.relu(self.bn3_ch4(self.conv3_ch4(x4)))
        x4 = self.pool2(x4)
        
        # æ‡‰ç”¨ç¬¬ä¸‰å±¤ dropout
        x1 = self.dropout_conv3(x1)
        x2 = self.dropout_conv3(x2)
        x3 = self.dropout_conv3(x3)
        x4 = self.dropout_conv3(x4)
        
        # ğŸ›¡ï¸ èåˆå±¤ + BatchNorm
        fsn3 = F.relu(self.bn_fusion(self.conv_fusion(fsn2)))
        fsn3 = self.pool2(fsn3)
        
        # Attention mechanism
        fsn3 = self.attention(fsn3, fsn3, fsn3)
        x1 = self.attention(fsn3, x1, x1)
        x2 = self.attention(fsn3, x2, x2)
        x3 = self.attention(fsn3, x3, x3)
        x4 = self.attention(fsn3, x4, x4)
        
        # Concatenate features
        concat = torch.cat([x1, x2, x3, x4], dim=1)  # [batch_size, 128, time]
        
        # âœ¨ å¢å¼·çš„ç‰¹å¾µèåˆï¼ˆåŠ å…¥å™ªè²å¢å¼·ï¼‰
        # 1. å…¨å±€å¹³å‡æ± åŒ–
        global_avg = torch.mean(concat, dim=2)  # [batch_size, 128]
        
        # 2. å…¨å±€æœ€å¤§æ± åŒ– 
        global_max = torch.max(concat, dim=2)[0]  # [batch_size, 128]
        
        # 3. ğŸ›¡ï¸ ç‰¹å¾µçµ„åˆ + éš¨æ©Ÿå™ªè²ï¼ˆè¨“ç·´æ™‚ï¼‰
        combined_features = global_avg + global_max  # [batch_size, 128]
        
        # ğŸ›¡ï¸ è¨“ç·´æ™‚æ·»åŠ é«˜æ–¯å™ªè²é˜²æ­¢éæ“¬åˆ
        if self.training:
            noise = torch.randn_like(combined_features) * 0.01
            combined_features = combined_features + noise
        
        # âœ¨ å¢å¼·çš„åˆ†é¡è·¯å¾‘ + å¤šå±¤æ¬¡æ­£å‰‡åŒ–
        x = F.relu(self.bn_fc1(self.fc1(combined_features)))  # [batch_size, 128]
        x = self.dropout1(x)
        
        x = F.relu(self.bn_fc2(self.fc2(x)))  # [batch_size, 64]
        x = self.dropout2(x)
        
        outputs = self.classifier(x)  # [batch_size, 2]
        
        # ğŸ”„ åªè¿”å›åˆ†é¡çµæœï¼Œæ²’æœ‰é‡å»ºè¼¸å‡º
        return outputs

class ApneaDataset(Dataset):
    def __init__(self, x, y):
        self.x = torch.FloatTensor(x)
        self.y = torch.LongTensor(y)
        
    def __len__(self):
        return len(self.x)
    
    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

def load_apnea_data():
    """
    è¼‰å…¥è™•ç†å¥½çš„ç¡çœ å‘¼å¸ä¸­æ­¢æ•¸æ“š
    é æœŸæ•¸æ“šæ ¼å¼: [batch_size, 1024, 4]
    """
    try:
        base_dir = r"C:\python\Apnea\physionet\split_data"
        
        # è¼‰å…¥æ•¸æ“š
        x_train = np.load(os.path.join(base_dir, "trainX.npy"))
        y_train = np.load(os.path.join(base_dir, "trainY.npy"))
        x_val = np.load(os.path.join(base_dir, "valX.npy"))
        y_val = np.load(os.path.join(base_dir, "valY.npy"))
        x_test = np.load(os.path.join(base_dir, "testX.npy"))
        y_test = np.load(os.path.join(base_dir, "testY.npy"))
        
        print(f"æ•¸æ“šè¼‰å…¥æˆåŠŸ!")
        print(f"è¨“ç·´é›†: {x_train.shape}, æ¨™ç±¤: {y_train.shape}")
        print(f"é©—è­‰é›†: {x_val.shape}, æ¨™ç±¤: {y_val.shape}")
        print(f"æ¸¬è©¦é›†: {x_test.shape}, æ¨™ç±¤: {y_test.shape}")
        print(f"æ¨™ç±¤ç¯„åœ: {np.unique(y_train)}")
        
        y_train = y_train.astype(int)
        y_val = y_val.astype(int)
        y_test = y_test.astype(int)
        
        return x_train, y_train, x_val, y_val, x_test, y_test
        
    except Exception as e:
        print(f"æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
        print("è«‹ç¢ºä¿æ•¸æ“šæ–‡ä»¶å­˜åœ¨æ–¼æŒ‡å®šè·¯å¾‘")
        return None

def train_model_classification_only(model, train_loader, val_loader, optimizer, num_epochs, device, early_stopping_patience=10):
    """
    ç´”åˆ†é¡è¨“ç·´ - æ²’æœ‰é‡å»ºæå¤± + å…¨é¢æ­£å‰‡åŒ–ç­–ç•¥
    
    Args:
        early_stopping_patience: æ—©åœè€å¿ƒå€¼ï¼Œé©—è­‰æå¤±é€£çºŒå¤šå°‘å€‹epochä¸æ”¹å–„å°±åœæ­¢
    """
    best_val_acc = 0.0
    best_val_loss = float('inf')
    patience_counter = 0
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
    
    # åªä½¿ç”¨åˆ†é¡æå¤±
    classification_criterion = nn.CrossEntropyLoss()
    
    # ğŸ›¡ï¸ å­¸ç¿’ç‡èª¿åº¦å™¨ï¼šç•¶é©—è­‰æå¤±åœæ­¢æ”¹å–„æ™‚é™ä½å­¸ç¿’ç‡
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6
    )
    
    # ğŸ›¡ï¸ æ··åˆç²¾åº¦è¨“ç·´ï¼ˆå¦‚æœæœ‰ GPUï¼‰
    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None
    
    print(f"é–‹å§‹è¨“ç·´ {num_epochs} epochs (å«æ—©åœæ©Ÿåˆ¶ï¼Œè€å¿ƒå€¼={early_stopping_patience})")
    print(f"ğŸ›¡ï¸ æ­£å‰‡åŒ–ç­–ç•¥ï¼šBatchNorm + åˆ†å±¤Dropout + å­¸ç¿’ç‡èª¿åº¦ + é«˜æ–¯å™ªè²")
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for inputs, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', total=len(train_loader)):
            inputs, targets = inputs.to(device), targets.to(device)
            
            optimizer.zero_grad()
            
            # ğŸ›¡ï¸ æ··åˆç²¾åº¦è¨“ç·´ï¼ˆæ›´é«˜æ•ˆ + éš±å¼æ­£å‰‡åŒ–ï¼‰
            if scaler is not None:
                with torch.cuda.amp.autocast():
                    outputs = model(inputs)  # åªæœ‰åˆ†é¡è¼¸å‡º
                    cls_loss = classification_criterion(outputs, targets)
                    
                    # ğŸ›¡ï¸ å¢å¼·çš„L2æ­£å‰‡åŒ–ï¼šåˆ†å±¤æ¬Šé‡è¡°æ¸›
                    l2_reg = torch.tensor(0., device=device)
                    for name, param in model.named_parameters():
                        if 'conv' in name:
                            l2_reg += torch.norm(param) * 0.5  # å·ç©å±¤è¼ƒè¼•çš„æ­£å‰‡åŒ–
                        elif 'fc' in name or 'classifier' in name:
                            l2_reg += torch.norm(param) * 1.0  # å…¨é€£æ¥å±¤è¼ƒé‡çš„æ­£å‰‡åŒ–
                    
                    # ç¸½æå¤±ï¼ˆæ²’æœ‰é‡å»ºæå¤±ï¼ï¼‰
                    loss = cls_loss + model.weight_decay * l2_reg
                
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                # CPU æˆ–ç„¡æ··åˆç²¾åº¦
                outputs = model(inputs)
                cls_loss = classification_criterion(outputs, targets)
                
                # ğŸ›¡ï¸ å¢å¼·çš„L2æ­£å‰‡åŒ–ï¼šåˆ†å±¤æ¬Šé‡è¡°æ¸›
                l2_reg = torch.tensor(0., device=device)
                for name, param in model.named_parameters():
                    if 'conv' in name:
                        l2_reg += torch.norm(param) * 0.5
                    elif 'fc' in name or 'classifier' in name:
                        l2_reg += torch.norm(param) * 1.0
                
                loss = cls_loss + model.weight_decay * l2_reg
                loss.backward()
                
                # ğŸ›¡ï¸ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += targets.size(0)
            train_correct += predicted.eq(targets).sum().item()
            
        train_loss = train_loss / len(train_loader)
        train_acc = 100. * train_correct / train_total
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)  # åªæœ‰åˆ†é¡è¼¸å‡º
                
                cls_loss = classification_criterion(outputs, targets)
                
                l2_reg = torch.tensor(0., device=device)
                for param in model.parameters():
                    l2_reg += torch.norm(param)
                
                loss = cls_loss + model.weight_decay * l2_reg
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += targets.size(0)
                val_correct += predicted.eq(targets).sum().item()
                
        val_loss = val_loss / len(val_loader)
        val_acc = 100. * val_correct / val_total
        
        # Save history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        # ğŸ›¡ï¸ å­¸ç¿’ç‡èª¿åº¦
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print(f'Learning Rate: {current_lr:.2e}')
        
        # Save best model (åŸºæ–¼é©—è­‰æº–ç¢ºç‡)
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_val_loss = val_loss
            patience_counter = 0
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'val_acc': val_acc,
                'val_loss': val_loss,
                'dropout_rate': model.dropout_rate
            }, 'BAFNet_ClassificationOnly_best.pth')
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {val_acc:.2f}%)")
        else:
            patience_counter += 1
            
        # ğŸ›¡ï¸ å¢å¼·çš„æ—©åœï¼šè€ƒæ…®å­¸ç¿’ç‡ä¸‹é™
        if patience_counter >= early_stopping_patience:
            print(f"\nğŸ›‘ æ—©åœè§¸ç™¼ï¼é©—è­‰æº–ç¢ºç‡é€£çºŒ {early_stopping_patience} epochs æ²’æœ‰æ”¹å–„")
            print(f"æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
            break
            
        # ğŸ›¡ï¸ å¦‚æœå­¸ç¿’ç‡å¤ªå°ï¼Œä¹Ÿæå‰åœæ­¢
        if current_lr < 1e-6:
            print(f"\nğŸ›‘ å­¸ç¿’ç‡éå° ({current_lr:.2e})ï¼Œæå‰åœæ­¢è¨“ç·´")
            break
            
    print(f"\nâœ… è¨“ç·´å®Œæˆï¼æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
    return history

def main():
    print("=== BAFNet ç´”åˆ†é¡ç‰ˆæœ¬è¨“ç·´ ===")
    
    # è¼‰å…¥æ•¸æ“š
    apnea_data = load_apnea_data()
    if apnea_data is None:
        return
        
    x_train, y_train, x_val, y_val, x_test, y_test = apnea_data
    
    # Create datasets and dataloaders
    train_dataset = ApneaDataset(x_train, y_train)
    val_dataset = ApneaDataset(x_val, y_val)
    test_dataset = ApneaDataset(x_test, y_test)
    
    # ğŸ”§ èª¿æ•´ batch size èˆ‡ YourModel ä¸€è‡´
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64)
    test_loader = DataLoader(test_dataset, batch_size=64)
    
    # Initialize model with å¯èª¿çš„dropoutç‡
    # ğŸ›¡ï¸ æ ¹æ“šæ•¸æ“šé›†å¤§å°èª¿æ•´ dropoutï¼šè¼ƒå°æ•¸æ“šé›†éœ€è¦æ›´å¼·æ­£å‰‡åŒ–
    dataset_size = len(x_train)
    if dataset_size < 5000:
        dropout_rate = 0.5  # å°æ•¸æ“šé›†ï¼šè¼ƒå¼·æ­£å‰‡åŒ–
    elif dataset_size < 15000:
        dropout_rate = 0.3  # ä¸­æ•¸æ“šé›†ï¼šä¸­ç­‰æ­£å‰‡åŒ–  
    else:
        dropout_rate = 0.2  # å¤§æ•¸æ“šé›†ï¼šè¼ƒè¼•æ­£å‰‡åŒ–
        
    model = BAFNet_ClassificationOnly(input_shape=x_train.shape[1:], 
                                     weight=1e-4,  # ğŸ›¡ï¸ é™ä½æ¬Šé‡è¡°æ¸›
                                     dropout_rate=dropout_rate).to(device)
    
    # ğŸ›¡ï¸ å„ªåŒ–å™¨ï¼šæ·»åŠ æ¬Šé‡è¡°æ¸›
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    
    print(f"æ¨¡å‹åƒæ•¸æ•¸é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    print(f"è¨“ç·´æ¨£æœ¬æ•¸: {len(x_train):,}")
    print(f"æ¯ epoch æ‰¹æ¬¡æ•¸: {len(train_loader)}")
    print(f"ğŸ›¡ï¸ Dropout ç‡: {dropout_rate} (åŸºæ–¼æ•¸æ“šé›†å¤§å° {dataset_size:,})")
    
    # ğŸ¯ Epoch è¨­å®šèªªæ˜
    print("\nğŸ“Š Epoch è¨­å®šæŒ‡å—:")
    print("â€¢ å¿«é€Ÿæ¸¬è©¦: 5-10 epochs")
    print("â€¢ åˆæ­¥è¨“ç·´: 20-30 epochs") 
    print("â€¢ æ­£å¼è¨“ç·´: 50-100 epochs")
    print("â€¢ æ·±åº¦å„ªåŒ–: 100+ epochs")
    print(f"â€¢ ç•¶å‰è¨­å®š: 50 epochs (å«æ—©åœ)")
    print("ğŸ›¡ï¸ æ­£å‰‡åŒ–ç­–ç•¥ï¼šBatchNorm + åˆ†å±¤Dropout + å­¸ç¿’ç‡èª¿åº¦ + æ¢¯åº¦è£å‰ª + é«˜æ–¯å™ªè²")
    
    # Train model
    # ğŸ”§ èª¿æ•´ epochsï¼šç¡çœ å‘¼å¸ä¸­æ­¢æª¢æ¸¬å»ºè­° 50-100 epochs
    # åˆæœŸæ¸¬è©¦: 20 epochs, æ­£å¼è¨“ç·´: 50-100 epochs
    history = train_model_classification_only(model, train_loader, val_loader, optimizer, 
                                            num_epochs=50, device=device, 
                                            early_stopping_patience=10)
    
    # ç²å–æœ€ä½³é©—è­‰æº–ç¢ºç‡ç”¨æ–¼éæ“¬åˆæª¢æ¸¬
    best_val_acc = max(history['val_acc']) if history['val_acc'] else 0
    
    # Evaluate on test set
    model.eval()
    test_correct = 0
    test_total = 0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)  # åªæœ‰åˆ†é¡è¼¸å‡º
            _, predicted = outputs.max(1)
            test_total += targets.size(0)
            test_correct += predicted.eq(targets).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    
    test_acc = 100. * test_correct / test_total
    print(f'Test Accuracy: {test_acc:.2f}%')
    
    # Calculate metrics
    f1 = f1_score(all_targets, all_preds, average='binary')
    roc = roc_auc_score(all_targets, all_preds)
    
    print(f'F1 Score: {f1:.4f}')
    print(f'ROC AUC: {roc:.4f}')
    
    print("\n=== èˆ‡åŸç‰ˆ BAFNet çš„å·®ç•° ===")
    print("âœ… ç§»é™¤äº† 4 å€‹é‡å»ºè§£ç¢¼å™¨ (å¤§å¹…æ¸›å°‘åƒæ•¸)")
    print("âœ… ç§»é™¤äº†é‡å»ºæå¤± (å°ˆæ³¨åˆ†é¡)")
    print("âœ… å¢å¼·äº†åˆ†é¡é ­éƒ¨ (æ›´æ·±çš„ç¶²çµ¡)")
    print("ğŸ›¡ï¸ å…¨é¢æ­£å‰‡åŒ–ç­–ç•¥:")
    print("  â€¢ BatchNorm1d (æ‰€æœ‰å·ç©å±¤å’Œå…¨é€£æ¥å±¤)")
    print("  â€¢ åˆ†å±¤ Dropout (æ¼¸é€²å¼å¢å¼·)")
    print("  â€¢ Dropout1d (å·ç©å±¤å°ˆç”¨)")
    print("  â€¢ é«˜æ–¯å™ªè² (è¨“ç·´æ™‚ç‰¹å¾µå¢å¼·)")
    print("  â€¢ å­¸ç¿’ç‡èª¿åº¦ (ReduceLROnPlateau)")
    print("  â€¢ æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)")
    print("  â€¢ åˆ†å±¤æ¬Šé‡è¡°æ¸› (å·ç©å±¤vså…¨é€£æ¥å±¤)")
    print("  â€¢ æ··åˆç²¾åº¦è¨“ç·´ (GPUå¯ç”¨æ™‚)")
    print("  â€¢ AdamWå„ªåŒ–å™¨ (å…§å»ºæ¬Šé‡è¡°æ¸›)")
    print("âœ… èª¿æ•´ batch size èˆ‡ YourModel ä¸€è‡´")
    print("ğŸ¯ æ™ºèƒ½ dropout ç‡é¸æ“‡ (åŸºæ–¼æ•¸æ“šé›†å¤§å°)")
    
    print(f"\nğŸ† æœ€çµ‚æ¸¬è©¦æ€§èƒ½:")
    print(f"  â€¢ æº–ç¢ºç‡: {test_acc:.2f}%")
    print(f"  â€¢ F1åˆ†æ•¸: {f1:.4f}")
    print(f"  â€¢ ROC AUC: {roc:.4f}")
    print(f"  â€¢ æ¨¡å‹åƒæ•¸: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    print(f"  â€¢ Dropoutç‡: {model.dropout_rate}")
    
    # ğŸ›¡ï¸ éæ“¬åˆæª¢æ¸¬æç¤º
    if test_acc < best_val_acc * 0.95:  # æ¸¬è©¦æº–ç¢ºç‡æ¯”æœ€å¥½çš„é©—è­‰æº–ç¢ºç‡ä½ 5% ä»¥ä¸Š
        print("\nâš ï¸  å¯èƒ½å­˜åœ¨è¼•å¾®éæ“¬åˆï¼Œå»ºè­°:")
        print("  â€¢ å¢åŠ  dropout_rate")
        print("  â€¢ å¢åŠ æ•¸æ“šå¢å¼·")
        print("  â€¢ æ¸›å°‘æ¨¡å‹è¤‡é›œåº¦")
    else:
        print("\nâœ… æ¨¡å‹æ³›åŒ–è‰¯å¥½ï¼Œç„¡æ˜é¡¯éæ“¬åˆ")

if __name__ == "__main__":
    main()
